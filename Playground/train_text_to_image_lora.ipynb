{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook for training the text to image model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check diffuser version & Save model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"0.16.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_card(repo_id: str, images=None, base_model=str, dataset_name=str, repo_folder=None):\n",
    "    img_str = \"\"\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
    "        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
    "\n",
    "    yaml = f\"\"\"\n",
    "---\n",
    "license: creativeml-openrail-m\n",
    "base_model: {base_model}\n",
    "tags:\n",
    "- stable-diffusion\n",
    "- stable-diffusion-diffusers\n",
    "- text-to-image\n",
    "- diffusers\n",
    "- lora\n",
    "inference: true\n",
    "---\n",
    "    \"\"\"\n",
    "    model_card = f\"\"\"\n",
    "# LoRA text2image fine-tuning - {repo_id}\n",
    "These are LoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset. You can find some example images in the following. \\n\n",
    "{img_str}\n",
    "\"\"\"\n",
    "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
    "        f.write(yaml + model_card)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
